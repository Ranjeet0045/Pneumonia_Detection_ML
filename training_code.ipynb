{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd09SXlbU_LV",
        "outputId": "2f4f48bf-f9a6-4181-f1e3-1e48fabb8a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è Downloading COVID-19 Radiography Database (Dataset A)...\n",
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: Ranjeet0045\n",
            "Your Kaggle Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Dataset URL: https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database\n",
            "Downloading covid19-radiography-database.zip to ./covid19-radiography-database\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 778M/778M [00:12<00:00, 64.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è Downloading Chest X-Ray Pneumonia (Dataset B)...\n",
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: Ranjeet0045\n",
            "Your Kaggle Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Dataset URL: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
            "Downloading chest-xray-pneumonia.zip to ./chest-xray-pneumonia\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.29G/2.29G [00:32<00:00, 75.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "‚úÖ Downloads Complete! You should see two folders on the left sidebar.\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# STEP 1: INSTALL & DOWNLOAD DATA\n",
        "# ==========================================\n",
        "# 1. INSTALL THE LIBRARY FIRST (This fixes your error)\n",
        "!pip install opendatasets --quiet\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import opendatasets as od\n",
        "\n",
        "# 2. Download Dataset A (Adults/General)\n",
        "print(\"‚¨áÔ∏è Downloading COVID-19 Radiography Database (Dataset A)...\")\n",
        "od.download('https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database')\n",
        "\n",
        "# 3. Download Dataset B (Pediatric Booster)\n",
        "print(\"‚¨áÔ∏è Downloading Chest X-Ray Pneumonia (Dataset B)...\")\n",
        "od.download('https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia')\n",
        "\n",
        "print(\"\\n‚úÖ Downloads Complete! You should see two folders on the left sidebar.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 2: MERGE, BALANCE & SPLIT (70/20/10)\n",
        "# ==========================================\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# 1. Define Destination\n",
        "BASE_DIR = 'Final_Dataset'\n",
        "if os.path.exists(BASE_DIR): shutil.rmtree(BASE_DIR) # Clean reset\n",
        "\n",
        "# Create folder structure\n",
        "classes = ['NORMAL', 'PNEUMONIA']\n",
        "for split in ['train', 'test', 'val']: # Note: test is 20%, val is 10%\n",
        "    for cls in classes:\n",
        "        os.makedirs(os.path.join(BASE_DIR, split, cls), exist_ok=True)\n",
        "\n",
        "print(\"üìÇ Created folder structure. Gathering files...\")\n",
        "\n",
        "# 2. Collect All Normal Images (Adults + Kids)\n",
        "normal_files = []\n",
        "# From Dataset 1 (Adults)\n",
        "d1_norm = 'covid19-radiography-database/COVID-19_Radiography_Dataset/Normal'\n",
        "if os.path.exists(d1_norm):\n",
        "    normal_files += [os.path.join(d1_norm, f) for f in os.listdir(d1_norm) if f.lower().endswith('png')]\n",
        "\n",
        "# From Dataset 2 (Kids)\n",
        "d2_norm_train = 'chest-xray-pneumonia/chest_xray/train/NORMAL'\n",
        "d2_norm_test = 'chest-xray-pneumonia/chest_xray/test/NORMAL'\n",
        "if os.path.exists(d2_norm_train):\n",
        "    normal_files += [os.path.join(d2_norm_train, f) for f in os.listdir(d2_norm_train) if f.lower().endswith('jpeg')]\n",
        "if os.path.exists(d2_norm_test):\n",
        "    normal_files += [os.path.join(d2_norm_test, f) for f in os.listdir(d2_norm_test) if f.lower().endswith('jpeg')]\n",
        "\n",
        "# 3. Collect All Pneumonia Images (Lung Opacity + Viral + COVID + Kids Pneu)\n",
        "pneumonia_files = []\n",
        "\n",
        "# D1: Lung Opacity (Bacterial)\n",
        "d1_opaque = 'covid19-radiography-database/COVID-19_Radiography_Dataset/Lung_Opacity'\n",
        "if not os.path.exists(d1_opaque): d1_opaque = 'covid19-radiography-database/COVID-19_Radiography_Dataset/Lung Opacity'\n",
        "if os.path.exists(d1_opaque):\n",
        "    pneumonia_files += [os.path.join(d1_opaque, f) for f in os.listdir(d1_opaque) if f.lower().endswith('png')]\n",
        "\n",
        "# D1: Viral Pneumonia\n",
        "d1_viral = 'covid19-radiography-database/COVID-19_Radiography_Dataset/Viral Pneumonia'\n",
        "if os.path.exists(d1_viral):\n",
        "    pneumonia_files += [os.path.join(d1_viral, f) for f in os.listdir(d1_viral) if f.lower().endswith('png')]\n",
        "\n",
        "# D1: COVID (Treating as Pneumonia for training)\n",
        "d1_covid = 'covid19-radiography-database/COVID-19_Radiography_Dataset/COVID'\n",
        "if os.path.exists(d1_covid):\n",
        "    pneumonia_files += [os.path.join(d1_covid, f) for f in os.listdir(d1_covid) if f.lower().endswith('png')]\n",
        "\n",
        "# D2: Kids Pneumonia\n",
        "d2_pneu_train = 'chest-xray-pneumonia/chest_xray/train/PNEUMONIA'\n",
        "d2_pneu_test = 'chest-xray-pneumonia/chest_xray/test/PNEUMONIA'\n",
        "if os.path.exists(d2_pneu_train):\n",
        "    pneumonia_files += [os.path.join(d2_pneu_train, f) for f in os.listdir(d2_pneu_train) if f.lower().endswith('jpeg')]\n",
        "if os.path.exists(d2_pneu_test):\n",
        "    pneumonia_files += [os.path.join(d2_pneu_test, f) for f in os.listdir(d2_pneu_test) if f.lower().endswith('jpeg')]\n",
        "\n",
        "print(f\"üìä Found: {len(normal_files)} Normal vs {len(pneumonia_files)} Pneumonia\")\n",
        "\n",
        "# 4. Balance the Classes (Trim the larger one)\n",
        "if len(pneumonia_files) > len(normal_files):\n",
        "    print(f\"‚öñÔ∏è Balancing: Trimming Pneumonia from {len(pneumonia_files)} to {len(normal_files)}\")\n",
        "    random.shuffle(pneumonia_files)\n",
        "    pneumonia_files = pneumonia_files[:len(normal_files)]\n",
        "elif len(normal_files) > len(pneumonia_files):\n",
        "    print(f\"‚öñÔ∏è Balancing: Trimming Normal from {len(normal_files)} to {len(pneumonia_files)}\")\n",
        "    random.shuffle(normal_files)\n",
        "    normal_files = normal_files[:len(pneumonia_files)]\n",
        "\n",
        "# 5. Distribute Files (70% Train, 20% Test, 10% Val)\n",
        "def distribute(file_list, category):\n",
        "    random.shuffle(file_list)\n",
        "    total = len(file_list)\n",
        "\n",
        "    # Calculate cut-off points\n",
        "    train_end = int(0.70 * total)\n",
        "    test_end = int(0.90 * total) # 70% + 20% = 90%\n",
        "    # Remaining 10% (from 0.90 to 1.0) goes to Val\n",
        "\n",
        "    for i, f in enumerate(file_list):\n",
        "        if i < train_end: split = 'train'\n",
        "        elif i < test_end: split = 'test'\n",
        "        else: split = 'val'\n",
        "\n",
        "        shutil.copy(f, os.path.join(BASE_DIR, split, category, os.path.basename(f)))\n",
        "\n",
        "print(\"üöÄ Moving files... (This takes ~2-3 minutes)\")\n",
        "distribute(normal_files, 'NORMAL')\n",
        "distribute(pneumonia_files, 'PNEUMONIA')\n",
        "\n",
        "print(\"‚úÖ Data Merged & Split Successfully!\")\n",
        "print(\"   Check the 'Final_Dataset' folder on the left.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGvym2U1V781",
        "outputId": "55a0938e-20c3-43a4-f8d7-82889f0c0452"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Created folder structure. Gathering files...\n",
            "üìä Found: 1575 Normal vs 4265 Pneumonia\n",
            "‚öñÔ∏è Balancing: Trimming Pneumonia from 4265 to 1575\n",
            "üöÄ Moving files... (This takes ~2-3 minutes)\n",
            "‚úÖ Data Merged & Split Successfully!\n",
            "   Check the 'Final_Dataset' folder on the left.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 3: BUILD & TRAIN (BINARY MODEL)\n",
        "# ==========================================\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "\n",
        "# Configuration\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "DATA_DIR = 'Final_Dataset'\n",
        "\n",
        "print(\"‚öôÔ∏è Setting up Data Generators...\")\n",
        "\n",
        "# 1. TRAIN GENERATOR (With Augmentation)\n",
        "# We zoom and rotate images slightly so the model learns to recognize pneumonia from all angles\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    f'{DATA_DIR}/train',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary', # Binary because we only have Normal vs Pneumonia\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# 2. VALIDATION GENERATOR (No Augmentation, just rescaling)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    f'{DATA_DIR}/val',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# 3. BUILD THE CNN MODEL\n",
        "print(\"üèóÔ∏è Building the CNN Model...\")\n",
        "model = Sequential([\n",
        "    # Block 1\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    # Block 2\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    # Block 3\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    # Deep Features\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5), # Drops 50% of neurons to prevent memorization\n",
        "\n",
        "    # OUTPUT LAYER: 1 Neuron (0 = Normal, 1 = Pneumonia)\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 4. START TRAINING\n",
        "print(\"üöÄ Training Started... This will take about 10-15 minutes.\")\n",
        "print(\"   (You will see the accuracy increase with every Epoch)\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# 5. SAVE THE MODEL\n",
        "model.save('pneumonia_binary_model.h5')\n",
        "print(\"\\n‚úÖ SUCCESS! Model saved as 'pneumonia_binary_model.h5'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyaYTpKzZFDU",
        "outputId": "9cf667a3-e707-4262-894b-11f15faf720c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öôÔ∏è Setting up Data Generators...\n",
            "Found 2204 images belonging to 2 classes.\n",
            "Found 316 images belonging to 2 classes.\n",
            "üèóÔ∏è Building the CNN Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Training Started... This will take about 10-15 minutes.\n",
            "   (You will see the accuracy increase with every Epoch)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 858ms/step - accuracy: 0.7730 - loss: 2.7987 - val_accuracy: 0.5000 - val_loss: 46.5271\n",
            "Epoch 2/10\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 744ms/step - accuracy: 0.8275 - loss: 0.5173 - val_accuracy: 0.4684 - val_loss: 8.3447\n",
            "Epoch 3/10\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 736ms/step - accuracy: 0.8644 - loss: 0.3770 - val_accuracy: 0.5000 - val_loss: 19.9300\n",
            "Epoch 4/10\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 740ms/step - accuracy: 0.8625 - loss: 0.3394 - val_accuracy: 0.5000 - val_loss: 37.0925\n",
            "Epoch 5/10\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 745ms/step - accuracy: 0.8895 - loss: 0.2970 - val_accuracy: 0.5000 - val_loss: 24.6149\n",
            "Epoch 6/10\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 759ms/step - accuracy: 0.8718 - loss: 0.3365 - val_accuracy: 0.5063 - val_loss: 5.6056\n",
            "Epoch 7/10\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 753ms/step - accuracy: 0.9076 - loss: 0.2780 - val_accuracy: 0.6424 - val_loss: 3.8245\n",
            "Epoch 8/10\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 796ms/step - accuracy: 0.9035 - loss: 0.2743 - val_accuracy: 0.6867 - val_loss: 0.9690\n",
            "Epoch 9/10\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 766ms/step - accuracy: 0.8779 - loss: 0.2662 - val_accuracy: 0.8734 - val_loss: 0.3671\n",
            "Epoch 10/10\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 756ms/step - accuracy: 0.8974 - loss: 0.2881 - val_accuracy: 0.8924 - val_loss: 0.2448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ SUCCESS! Model saved as 'pneumonia_binary_model.h5'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# NEW STEP 2: MERGE EVERYTHING (NO TRIMMING)\n",
        "# ==========================================\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from tqdm import tqdm # Progress bar\n",
        "\n",
        "BASE_DIR = 'Final_Dataset_Full'\n",
        "if os.path.exists(BASE_DIR): shutil.rmtree(BASE_DIR)\n",
        "\n",
        "# Create structure\n",
        "for split in ['train', 'test', 'val']:\n",
        "    for cls in ['NORMAL', 'PNEUMONIA']:\n",
        "        os.makedirs(os.path.join(BASE_DIR, split, cls), exist_ok=True)\n",
        "\n",
        "print(\"üïµÔ∏è SEARCHING FOR IMAGES...\")\n",
        "\n",
        "# --- 1. DEFINE SOURCES ---\n",
        "# We map specific source folders to our target classes\n",
        "sources = [\n",
        "    # (Path to source folder, Target Class)\n",
        "\n",
        "    # DATASET A (Adults)\n",
        "    ('covid19-radiography-database/COVID-19_Radiography_Dataset/Normal', 'NORMAL'),\n",
        "    ('covid19-radiography-database/COVID-19_Radiography_Dataset/Lung_Opacity', 'PNEUMONIA'),\n",
        "    ('covid19-radiography-database/COVID-19_Radiography_Dataset/Viral Pneumonia', 'PNEUMONIA'),\n",
        "    ('covid19-radiography-database/COVID-19_Radiography_Dataset/COVID', 'PNEUMONIA'),\n",
        "\n",
        "    # DATASET B (Kids) - Train folder\n",
        "    ('chest-xray-pneumonia/chest_xray/train/NORMAL', 'NORMAL'),\n",
        "    ('chest-xray-pneumonia/chest_xray/train/PNEUMONIA', 'PNEUMONIA'),\n",
        "\n",
        "    # DATASET B (Kids) - Test/Val folders (We merge them all to reshuffle)\n",
        "    ('chest-xray-pneumonia/chest_xray/test/NORMAL', 'NORMAL'),\n",
        "    ('chest-xray-pneumonia/chest_xray/test/PNEUMONIA', 'PNEUMONIA'),\n",
        "    ('chest-xray-pneumonia/chest_xray/val/NORMAL', 'NORMAL'),\n",
        "    ('chest-xray-pneumonia/chest_xray/val/PNEUMONIA', 'PNEUMONIA'),\n",
        "]\n",
        "\n",
        "# --- 2. COLLECT FILES ---\n",
        "normal_files = []\n",
        "pneumonia_files = []\n",
        "\n",
        "for path, target_cls in sources:\n",
        "    if os.path.exists(path):\n",
        "        # Find all images\n",
        "        images = [os.path.join(path, f) for f in os.listdir(path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        count = len(images)\n",
        "        print(f\"   Found {count} images in: {path} -> {target_cls}\")\n",
        "\n",
        "        if target_cls == 'NORMAL':\n",
        "            normal_files.extend(images)\n",
        "        else:\n",
        "            pneumonia_files.extend(images)\n",
        "    else:\n",
        "        # Try fixing \"Lung_Opacity\" vs \"Lung Opacity\" naming issue\n",
        "        if \"Lung_Opacity\" in path:\n",
        "            alt_path = path.replace(\"Lung_Opacity\", \"Lung Opacity\")\n",
        "            if os.path.exists(alt_path):\n",
        "                images = [os.path.join(alt_path, f) for f in os.listdir(alt_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "                print(f\"   Found {len(images)} images in: {alt_path} -> {target_cls}\")\n",
        "                pneumonia_files.extend(images)\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è WARNING: Folder not found: {path}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è WARNING: Folder not found: {path}\")\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"üìä TOTAL NORMAL FOUND: {len(normal_files)}\")\n",
        "print(f\"üìä TOTAL PNEUMONIA FOUND: {len(pneumonia_files)}\")\n",
        "print(f\"üî• GRAND TOTAL: {len(normal_files) + len(pneumonia_files)}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# --- 3. DISTRIBUTE WITHOUT TRIMMING ---\n",
        "def distribute(file_list, category):\n",
        "    random.shuffle(file_list)\n",
        "    total = len(file_list)\n",
        "    train_end = int(0.70 * total)\n",
        "    test_end = int(0.90 * total)\n",
        "\n",
        "    print(f\"üöÄ Moving {total} {category} images...\")\n",
        "\n",
        "    for i, f in enumerate(tqdm(file_list)):\n",
        "        if i < train_end: split = 'train'\n",
        "        elif i < test_end: split = 'test'\n",
        "        else: split = 'val'\n",
        "\n",
        "        shutil.copy(f, os.path.join(BASE_DIR, split, category, os.path.basename(f)))\n",
        "\n",
        "distribute(normal_files, 'NORMAL')\n",
        "distribute(pneumonia_files, 'PNEUMONIA')\n",
        "\n",
        "print(\"\\n‚úÖ SUCCESS! Full dataset prepared in 'Final_Dataset_Full'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UklT-2tduML",
        "outputId": "4224f23f-5219-4710-b472-a633a110a106"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üïµÔ∏è SEARCHING FOR IMAGES...\n",
            "   Found 0 images in: covid19-radiography-database/COVID-19_Radiography_Dataset/Normal -> NORMAL\n",
            "   Found 0 images in: covid19-radiography-database/COVID-19_Radiography_Dataset/Lung_Opacity -> PNEUMONIA\n",
            "   Found 0 images in: covid19-radiography-database/COVID-19_Radiography_Dataset/Viral Pneumonia -> PNEUMONIA\n",
            "   Found 0 images in: covid19-radiography-database/COVID-19_Radiography_Dataset/COVID -> PNEUMONIA\n",
            "   Found 1341 images in: chest-xray-pneumonia/chest_xray/train/NORMAL -> NORMAL\n",
            "   Found 3875 images in: chest-xray-pneumonia/chest_xray/train/PNEUMONIA -> PNEUMONIA\n",
            "   Found 234 images in: chest-xray-pneumonia/chest_xray/test/NORMAL -> NORMAL\n",
            "   Found 390 images in: chest-xray-pneumonia/chest_xray/test/PNEUMONIA -> PNEUMONIA\n",
            "   Found 8 images in: chest-xray-pneumonia/chest_xray/val/NORMAL -> NORMAL\n",
            "   Found 8 images in: chest-xray-pneumonia/chest_xray/val/PNEUMONIA -> PNEUMONIA\n",
            "----------------------------------------\n",
            "üìä TOTAL NORMAL FOUND: 1583\n",
            "üìä TOTAL PNEUMONIA FOUND: 4273\n",
            "üî• GRAND TOTAL: 5856\n",
            "----------------------------------------\n",
            "üöÄ Moving 1583 NORMAL images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1583/1583 [00:12<00:00, 130.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Moving 4273 PNEUMONIA images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4273/4273 [00:01<00:00, 2751.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ SUCCESS! Full dataset prepared in 'Final_Dataset_Full'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# FINAL FIX: AUTO-DISCOVERY MERGE (SMART SCAN)\n",
        "# ==========================================\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "BASE_DIR = 'Final_Dataset_Full'\n",
        "if os.path.exists(BASE_DIR): shutil.rmtree(BASE_DIR)\n",
        "\n",
        "# Create destination folders\n",
        "for split in ['train', 'test', 'val']:\n",
        "    for cls in ['NORMAL', 'PNEUMONIA']:\n",
        "        os.makedirs(os.path.join(BASE_DIR, split, cls), exist_ok=True)\n",
        "\n",
        "print(\"üïµÔ∏è STARTING DEEP SCAN FOR ALL IMAGES...\")\n",
        "\n",
        "# Lists to hold file paths\n",
        "normal_files = []\n",
        "pneumonia_files = []\n",
        "\n",
        "# We walk through every single folder in your Colab workspace\n",
        "for root, dirs, files in os.walk('.'):\n",
        "    # Skip the destination folder itself to avoid loops\n",
        "    if BASE_DIR in root or '.config' in root:\n",
        "        continue\n",
        "\n",
        "    folder_name = os.path.basename(root).lower()\n",
        "\n",
        "    # 1. IDENTIFY PNEUMONIA FOLDERS\n",
        "    # We look for keywords: 'covid', 'lung_opacity', 'viral pneumonia', or just 'pneumonia'\n",
        "    if folder_name in ['covid', 'lung_opacity', 'lung opacity', 'viral pneumonia', 'pneumonia']:\n",
        "        # Found a sickness folder! Add images to Pneumonia list\n",
        "        images = [os.path.join(root, f) for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        if len(images) > 0:\n",
        "            print(f\"   ‚úÖ Found PNEUMONIA data: {len(images)} images in '{root}'\")\n",
        "            pneumonia_files.extend(images)\n",
        "\n",
        "    # 2. IDENTIFY NORMAL FOLDERS\n",
        "    elif folder_name == 'normal':\n",
        "        # Found a healthy folder! Add images to Normal list\n",
        "        images = [os.path.join(root, f) for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        if len(images) > 0:\n",
        "            print(f\"   ‚úÖ Found NORMAL data:    {len(images)} images in '{root}'\")\n",
        "            normal_files.extend(images)\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"üìä TOTAL NORMAL FOUND:    {len(normal_files)}\")\n",
        "print(f\"üìä TOTAL PNEUMONIA FOUND: {len(pneumonia_files)}\")\n",
        "print(f\"üî• GRAND TOTAL:           {len(normal_files) + len(pneumonia_files)}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "if len(normal_files) < 5000:\n",
        "    print(\"‚ö†Ô∏è WARNING: Still low on images. Did you run the 'Download' step?\")\n",
        "else:\n",
        "    print(\"üöÄ SUCCESS! Starting merge...\")\n",
        "\n",
        "# --- DISTRIBUTE FILES ---\n",
        "def distribute(file_list, category):\n",
        "    random.shuffle(file_list)\n",
        "    total = len(file_list)\n",
        "    train_end = int(0.70 * total)\n",
        "    test_end = int(0.90 * total)\n",
        "\n",
        "    for i, f in enumerate(tqdm(file_list)):\n",
        "        if i < train_end: split = 'train'\n",
        "        elif i < test_end: split = 'test'\n",
        "        else: split = 'val'\n",
        "\n",
        "        # Handle duplicate filenames by renaming if necessary\n",
        "        dest_path = os.path.join(BASE_DIR, split, category, os.path.basename(f))\n",
        "        if os.path.exists(dest_path):\n",
        "            filename, ext = os.path.splitext(os.path.basename(f))\n",
        "            dest_path = os.path.join(BASE_DIR, split, category, f\"{filename}_dup{i}{ext}\")\n",
        "\n",
        "        shutil.copy(f, dest_path)\n",
        "\n",
        "distribute(normal_files, 'NORMAL')\n",
        "distribute(pneumonia_files, 'PNEUMONIA')\n",
        "\n",
        "print(\"\\n‚úÖ FULL DATASET READY IN 'Final_Dataset_Full'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkJZ37KPeWK9",
        "outputId": "489d6b88-79ae-429c-9d77-9853f37dd1b1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üïµÔ∏è STARTING DEEP SCAN FOR ALL IMAGES...\n",
            "   ‚úÖ Found PNEUMONIA data: 1102 images in './Final_Dataset/train/PNEUMONIA'\n",
            "   ‚úÖ Found NORMAL data:    1102 images in './Final_Dataset/train/NORMAL'\n",
            "   ‚úÖ Found PNEUMONIA data: 158 images in './Final_Dataset/val/PNEUMONIA'\n",
            "   ‚úÖ Found NORMAL data:    158 images in './Final_Dataset/val/NORMAL'\n",
            "   ‚úÖ Found PNEUMONIA data: 315 images in './Final_Dataset/test/PNEUMONIA'\n",
            "   ‚úÖ Found NORMAL data:    315 images in './Final_Dataset/test/NORMAL'\n",
            "   ‚úÖ Found PNEUMONIA data: 3875 images in './chest-xray-pneumonia/chest_xray/train/PNEUMONIA'\n",
            "   ‚úÖ Found NORMAL data:    1341 images in './chest-xray-pneumonia/chest_xray/train/NORMAL'\n",
            "   ‚úÖ Found PNEUMONIA data: 3875 images in './chest-xray-pneumonia/chest_xray/__MACOSX/chest_xray/train/PNEUMONIA'\n",
            "   ‚úÖ Found NORMAL data:    1341 images in './chest-xray-pneumonia/chest_xray/__MACOSX/chest_xray/train/NORMAL'\n",
            "   ‚úÖ Found PNEUMONIA data: 8 images in './chest-xray-pneumonia/chest_xray/__MACOSX/chest_xray/val/PNEUMONIA'\n",
            "   ‚úÖ Found NORMAL data:    8 images in './chest-xray-pneumonia/chest_xray/__MACOSX/chest_xray/val/NORMAL'\n",
            "   ‚úÖ Found PNEUMONIA data: 390 images in './chest-xray-pneumonia/chest_xray/__MACOSX/chest_xray/test/PNEUMONIA'\n",
            "   ‚úÖ Found NORMAL data:    234 images in './chest-xray-pneumonia/chest_xray/__MACOSX/chest_xray/test/NORMAL'\n",
            "   ‚úÖ Found PNEUMONIA data: 8 images in './chest-xray-pneumonia/chest_xray/val/PNEUMONIA'\n",
            "   ‚úÖ Found NORMAL data:    8 images in './chest-xray-pneumonia/chest_xray/val/NORMAL'\n",
            "   ‚úÖ Found PNEUMONIA data: 390 images in './chest-xray-pneumonia/chest_xray/test/PNEUMONIA'\n",
            "   ‚úÖ Found NORMAL data:    234 images in './chest-xray-pneumonia/chest_xray/test/NORMAL'\n",
            "   ‚úÖ Found PNEUMONIA data: 3875 images in './chest-xray-pneumonia/chest_xray/chest_xray/train/PNEUMONIA'\n",
            "   ‚úÖ Found NORMAL data:    1341 images in './chest-xray-pneumonia/chest_xray/chest_xray/train/NORMAL'\n",
            "   ‚úÖ Found PNEUMONIA data: 8 images in './chest-xray-pneumonia/chest_xray/chest_xray/val/PNEUMONIA'\n",
            "   ‚úÖ Found NORMAL data:    8 images in './chest-xray-pneumonia/chest_xray/chest_xray/val/NORMAL'\n",
            "   ‚úÖ Found PNEUMONIA data: 390 images in './chest-xray-pneumonia/chest_xray/chest_xray/test/PNEUMONIA'\n",
            "   ‚úÖ Found NORMAL data:    234 images in './chest-xray-pneumonia/chest_xray/chest_xray/test/NORMAL'\n",
            "----------------------------------------\n",
            "üìä TOTAL NORMAL FOUND:    6324\n",
            "üìä TOTAL PNEUMONIA FOUND: 14394\n",
            "üî• GRAND TOTAL:           20718\n",
            "----------------------------------------\n",
            "üöÄ SUCCESS! Starting merge...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6324/6324 [00:21<00:00, 296.10it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14394/14394 [00:12<00:00, 1175.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ FULL DATASET READY IN 'Final_Dataset_Full'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 3: TRAIN ON FULL CLEAN DATASET (15k Images)\n",
        "# ==========================================\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "\n",
        "# Configuration\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "DATA_DIR = 'Final_Dataset_Full' # Pointing to the clean, merged dataset\n",
        "\n",
        "print(f\"üöÄ Preparing to train on cleaned dataset: {DATA_DIR}\")\n",
        "\n",
        "# 1. DATA GENERATORS\n",
        "# Augmentation helps the model generalize better on this large dataset\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    f'{DATA_DIR}/train',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    f'{DATA_DIR}/val',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# 2. BUILD ROBUST CNN MODEL\n",
        "model = Sequential([\n",
        "    # Block 1\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    # Block 2\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    # Block 3\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    # Block 4 (Deep Features)\n",
        "    Conv2D(256, (3,3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    # Classification Head\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5), # Prevents overfitting\n",
        "    Dense(1, activation='sigmoid') # Binary Output (0=Normal, 1=Pneumonia)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 3. START TRAINING\n",
        "# Since we have ~15,000 images, 5 Epochs is plenty to get high accuracy without waiting hours.\n",
        "print(\"üî• Starting Training... (This will take approx 20-30 mins)\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=5\n",
        ")\n",
        "\n",
        "# 4. SAVE MODEL\n",
        "model.save('pneumonia_mega_model.h5')\n",
        "print(\"\\n‚úÖ SUCCESS! Model saved as 'pneumonia_mega_model.h5'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70xwKxh2f518",
        "outputId": "13d17762-0f81-4c5d-d36a-42a38d5b8f83"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Preparing to train on cleaned dataset: Final_Dataset_Full\n",
            "Found 10373 images belonging to 2 classes.\n",
            "Found 1482 images belonging to 2 classes.\n",
            "üî• Starting Training... (This will take approx 20-30 mins)\n",
            "Epoch 1/5\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 711ms/step - accuracy: 0.8284 - loss: 1.6279 - val_accuracy: 0.6788 - val_loss: 18.7159\n",
            "Epoch 2/5\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 668ms/step - accuracy: 0.9079 - loss: 0.2412 - val_accuracy: 0.6862 - val_loss: 3.6997\n",
            "Epoch 3/5\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 662ms/step - accuracy: 0.9124 - loss: 0.2273 - val_accuracy: 0.9231 - val_loss: 0.2042\n",
            "Epoch 4/5\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 660ms/step - accuracy: 0.9316 - loss: 0.2070 - val_accuracy: 0.6788 - val_loss: 13.5913\n",
            "Epoch 5/5\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 659ms/step - accuracy: 0.9282 - loss: 0.2007 - val_accuracy: 0.8900 - val_loss: 0.3023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ SUCCESS! Model saved as 'pneumonia_mega_model.h5'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "import numpy\n",
        "import PIL # This is Pillow\n",
        "import opendatasets\n",
        "import tqdm\n",
        "import matplotlib\n",
        "\n",
        "print(\"Copy these lines below:\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"tensorflow=={tensorflow.__version__}\")\n",
        "print(f\"numpy=={numpy.__version__}\")\n",
        "print(f\"Pillow=={PIL.__version__}\")\n",
        "print(f\"opendatasets=={opendatasets.__version__}\")\n",
        "print(f\"tqdm=={tqdm.__version__}\")\n",
        "print(f\"matplotlib=={matplotlib.__version__}\")\n",
        "print(\"flask\") # Flask is standard, version doesn't strictly matter\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AhkYDXmoyXi",
        "outputId": "20791d1f-fe63-4021-ee0a-98c552d55ab1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copy these lines below:\n",
            "------------------------------\n",
            "tensorflow==2.19.0\n",
            "numpy==2.0.2\n",
            "Pillow==11.3.0\n",
            "opendatasets==0.1.22\n",
            "tqdm==4.67.1\n",
            "matplotlib==3.10.0\n",
            "flask\n",
            "------------------------------\n"
          ]
        }
      ]
    }
  ]
}